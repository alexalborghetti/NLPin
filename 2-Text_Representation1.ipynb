{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+wrB8mVut8eoaVbw2WzrV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Representation (I)"],"metadata":{"id":"cz3b-7KOEWPg"}},{"cell_type":"markdown","source":["In our previous notebook, we covered text preprocessing techniques essential for cleaning and preparing raw text data. Now, we will explore how to represent this preprocessed text in a format suitable for machine learning models.\n","Effective text representation is fundamental for developing robust NLP applications. In this notebook, we will explore:\n","- Bag of Words (BoW): A straightforward method using word counts for representation.\n","- TF-IDF (Term Frequency-Inverse Document Frequency): A technique that evaluates words based on their importance in documents.\n","\n","Later on, we will discuss more advanced techniques such as word embeddings.\n","By the end of this session, you'll grasp these essential text representation techniques and their practical implementation. We will utilize the scikit-learn library to implement and apply these text representation techniques (https://scikit-learn.org/stable/)."],"metadata":{"id":"ZrgUub4iEbye"}},{"cell_type":"markdown","source":["## Bag of Words (BoW)"],"metadata":{"id":"WAD3imr_FDeG"}},{"cell_type":"markdown","source":["The Bag of Words (BoW) model is one of the simplest text representation techniques in NLP. It transforms text into fixed-length vectors by counting the occurrence of each word in the text. This approach ignores grammar and word order, focusing solely on word frequency. Let's walk through the implementation of the BoW model using the `scikit-learn` library and, more specifically, the `CountVectorizer` object."],"metadata":{"id":"mwqfOgT5FF09"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"An1qf_9vDVNg","executionInfo":{"status":"ok","timestamp":1719662909656,"user_tz":-120,"elapsed":1502,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","source":["We will consider a couple of sentences as examples.\n"],"metadata":{"id":"MGUp9bpChhnJ"}},{"cell_type":"code","source":["sample_sentences = [\n","    \"My bunny Giovanna eats fennel and celery.\",\n","    \"Celery is a crunchy vegetable.\"\n","]"],"metadata":{"id":"TntnLcptheoa","executionInfo":{"status":"ok","timestamp":1719662909657,"user_tz":-120,"elapsed":11,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["`CountVectorizer` is an object in `scikit-learn` that converts a collection of text documents to a matrix of token counts. A token for CountVectorizer is a single unit of text, typically a word, that is counted in the text documents."],"metadata":{"id":"U5bm7P-ajFeX"}},{"cell_type":"code","source":["vectorizer = CountVectorizer()"],"metadata":{"id":"FMl2poxoh5c5","executionInfo":{"status":"ok","timestamp":1719662909657,"user_tz":-120,"elapsed":10,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["We fit the `CountVectorizer` to the sample sentences and transform these into a BoW representation."],"metadata":{"id":"AHQ_-zTTjQhS"}},{"cell_type":"code","source":["X = vectorizer.fit_transform(sample_sentences)"],"metadata":{"id":"6tdrzHnDjP9L","executionInfo":{"status":"ok","timestamp":1719662909658,"user_tz":-120,"elapsed":11,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["`X` is a sparse matrix containing the token counts for each document generated by `CountVectorizer`. We use `X.toarray()` to convert this sparse matrix into a dense array for easier manipulation and visualization."],"metadata":{"id":"R93KYUvTlAJM"}},{"cell_type":"code","source":["X_dense = X.toarray()\n","feature_names = vectorizer.get_feature_names_out()\n","df_bow = pd.DataFrame(X_dense, columns=feature_names)"],"metadata":{"id":"M0f47U5EjYWx","executionInfo":{"status":"ok","timestamp":1719662909658,"user_tz":-120,"elapsed":11,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Let's see the result."],"metadata":{"id":"XDp1fmTolTxJ"}},{"cell_type":"code","source":["print(\"Result: \\n\\n\", df_bow)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6vifR9IlGIi","executionInfo":{"status":"ok","timestamp":1719662909658,"user_tz":-120,"elapsed":10,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"551ef044-c7af-4f3a-ff26-aaee0e20fb23"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Result: \n","\n","    and  bunny  celery  crunchy  eats  fennel  giovanna  is  my  vegetable\n","0    1      1       1        0     1       1         1   0   1          0\n","1    0      0       1        1     0       0         0   1   0          1\n"]}]},{"cell_type":"markdown","source":["The result is a dense matrix where each row represents a sentence, and each column corresponds to a lowercased token (word). The values indicate the frequency of each word in the respective sentence. For example, in the first sentence, \"and\", \"bunny\", \"celery\", \"eats\", \"fennel\", and \"giovanna\" each appear once, while \"crunchy\" and \"vegetable\" do not appear. In the second sentence, \"celery\", \"crunchy\", and \"is\" each appear once, while the other words do not."],"metadata":{"id":"WX3BeBXIlW_y"}},{"cell_type":"markdown","source":["**Bonus**: We can combine the BoW approach with the lemmatization technique learned in the previous notebook. What benefits could be obtained? Let's consider these two sentences."],"metadata":{"id":"_pMo-Psml6KY"}},{"cell_type":"code","source":["sample_sentences = [\n","    \"Giovanna loves to hop\",\n","    \"Giovanna is hopping\"\n","]"],"metadata":{"id":"oYMP0ePklXaa","executionInfo":{"status":"ok","timestamp":1719662909658,"user_tz":-120,"elapsed":8,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Without lemmatization, everything is as we have already seen.\""],"metadata":{"id":"JD32K2SQnQR8"}},{"cell_type":"code","source":["X = vectorizer.fit_transform(sample_sentences)\n","X_dense = X.toarray()\n","feature_names = vectorizer.get_feature_names_out()\n","df_bow = pd.DataFrame(X_dense, columns=feature_names)\n","print(\"Result: \\n\\n\", df_bow)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJzQeZ8inPyC","executionInfo":{"status":"ok","timestamp":1719662909659,"user_tz":-120,"elapsed":7,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"63264c31-9677-4844-ef8d-4f9170159b48"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Result: \n","\n","    giovanna  hop  hopping  is  loves  to\n","0         1    1        0   0      1   1\n","1         1    0        1   1      0   0\n"]}]},{"cell_type":"markdown","source":["In order to try the combination between BoW and Lemmatization, let's import `nltk`."],"metadata":{"id":"e-dCMp2UnZ2e"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import download\n","download('punkt')\n","download('wordnet')\n","download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvIrI-byoA-L","executionInfo":{"status":"ok","timestamp":1719662910069,"user_tz":-120,"elapsed":415,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"42355b26-85cc-4048-f56d-ac1ac5f1c3a2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["We now define the `LemmaTokenizer` class: it lemmatizes and tokenizes text, reducing words to their base forms to improve text representation in Bag of Words (BoW). The `LemmaTokenizer` class inherits from `BaseEstimator` and `TransformerMixin`, which are utility classes in scikit-learn that facilitate the creation of custom transformers and ensure compatibility with scikit-learn's pipeline framework. (It is assumed that the reader already knows how classes work in Python. Explaining how classes work in Python is not the focus of this notebook.)"],"metadata":{"id":"c2Gg37eupqew"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class LemmaTokenizer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.wnl = WordNetLemmatizer()\n","\n","    def get_wordnet_pos(self, word):\n","        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","        tag = nltk.pos_tag([word])[0][1][0].upper()\n","        tag_dict = {\"J\": wordnet.ADJ,\n","                    \"N\": wordnet.NOUN,\n","                    \"V\": wordnet.VERB,\n","                    \"R\": wordnet.ADV}\n","        return tag_dict.get(tag, wordnet.NOUN)\n","\n","    def __call__(self, doc):\n","        return [self.wnl.lemmatize(t, self.get_wordnet_pos(t)) for t in word_tokenize(doc)]"],"metadata":{"id":"vB-43g1SnUrS","executionInfo":{"status":"ok","timestamp":1719662910069,"user_tz":-120,"elapsed":4,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["We can use `LemmaTokenizer` with `CountVectorizer` to preprocess text with lemmatization before creating the BoW matrix."],"metadata":{"id":"9R4xlNC0pxgj"}},{"cell_type":"code","source":["vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n","X = vectorizer.fit_transform(sample_sentences)\n","X_dense = X.toarray()\n","feature_names = vectorizer.get_feature_names_out()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dppa-rexn0F9","executionInfo":{"status":"ok","timestamp":1719662915044,"user_tz":-120,"elapsed":4978,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"d2de9535-7309-415e-ec58-a41ab59a7e1d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Let's see the result."],"metadata":{"id":"ST0oht9Ep9dI"}},{"cell_type":"code","source":["df_bow = pd.DataFrame(X_dense, columns=feature_names)\n","print(\"Result: \\n\\n\", df_bow)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EkvCX4Aap81a","executionInfo":{"status":"ok","timestamp":1719662915044,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"b618cecd-1e12-4233-f1b7-eb2df390e48b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Result: \n","\n","    be  giovanna  hop  love  to\n","0   0         1    1     1   1\n","1   1         1    1     0   0\n"]}]},{"cell_type":"markdown","source":["We can note that with lemmatization, words like \"hopping\" are reduced to their base forms \"hop,\" reducing feature dimensionality and merging similar terms. This simplifies the model and improves consistency."],"metadata":{"id":"_NgLWClDqGSR"}},{"cell_type":"markdown","source":["## TF-IDF"],"metadata":{"id":"MbA1_l2TnSfh"}},{"cell_type":"markdown","source":["After discussing the BoW model, let's now explore TF-IDF (Term Frequency-Inverse Document Frequency). TF-IDF addresses some limitations of BoW, such as improving relevance by weighting words based on their document frequency; however, certain limitations, such as the disregard for word order and the lack of semantic understanding, still remain. In other words, while BoW counts how often words appear in a document, TF-IDF also factors in how rare those words are in general. Indeed, the formula for TF-IDF combines two components: Term Frequency (TF) and Inverse Document Frequency (IDF). TF measures how frequently a term appears in a document, normalized by the total number of terms in that document. IDF, on the other hand, measures how rare or common a term is across all documents in the corpus. By multiplying TF and IDF together, TF-IDF assigns higher weights to terms that are frequent in the document but rare across the corpus, helping to identify words that are uniquely significant to a specific document.\n","In mathematical terms:\n","\n","- $TF(t, s) = \\frac{\\mbox{number of times term t appears in sentence s}}{\\mbox{total number of terms in s}}$\n","\n","- $IDF(t) = \\log \\bigg{(}\\frac{\\mbox{total number of sentences}}{\\mbox{1 + number of sentences containing t}}\\bigg{)}$\n","\n","- $\\mbox{TF-IDF}(t) = TF(t) \\times IDF(t)$.\n","\n","This helps to emphasize words that are distinctively important to a specific document compared to others in the corpus.\n","\n","Again, we can implement TF-IDF using the `sklearn` library. Specifically, we'll utilize the `TfidfVectorizer` object.\n","\n"," It's important to note that the `TfidfVectorizer` does not precisely use the previous formulas. Instead, it adjusts IDF calculations to ensure numerical stability and might incorporate additional optimizations for performance and effectiveness in real-world applications. For more details, refer to the documentation. However, the basic idea is the same."],"metadata":{"id":"-KTR0VVqnVOh"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer()"],"metadata":{"id":"APIPYMbpqBxJ","executionInfo":{"status":"ok","timestamp":1719662915045,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Let's consider the previous sample sentences."],"metadata":{"id":"F2W-7Pi_ooo4"}},{"cell_type":"code","source":["sample_sentences = [\n","    \"Giovanna loves to hop.\",\n","    \"Giovanna is hopping.\"\n","]"],"metadata":{"id":"qsSf5LlinUA5","executionInfo":{"status":"ok","timestamp":1719662915045,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["`TfIdfVectorizer`, like the `CountVectorizer`, is an object in `scikit-learn` that converts a collection of text documents to a matrix. However, in this case, the weight assigned to each token is determined by the TF-IDF metric rather than its count.\""],"metadata":{"id":"WMWkdyAEpIBh"}},{"cell_type":"code","source":["X_tfidf = tfidf_vectorizer.fit_transform(sample_sentences)"],"metadata":{"id":"Iv72OCA-owzw","executionInfo":{"status":"ok","timestamp":1719662915046,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Let's display the result."],"metadata":{"id":"kRHj9x7Mpgze"}},{"cell_type":"code","source":["X_tfidf_dense = X_tfidf.toarray()\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","df_bow = pd.DataFrame(X_tfidf_dense, columns=feature_names)\n","print(\"Result: \\n\\n\", df_bow)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bbS-ahFo1zo","executionInfo":{"status":"ok","timestamp":1719662915046,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alex Alborghetti","userId":"02552696889110785741"}},"outputId":"10222bba-0748-427e-f63a-0e56bd3490e5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Result: \n","\n","    giovanna       hop   hopping        is     loves        to\n","0  0.379978  0.534046  0.000000  0.000000  0.534046  0.534046\n","1  0.449436  0.000000  0.631667  0.631667  0.000000  0.000000\n"]}]},{"cell_type":"markdown","source":["Is the result as expected? Let's consider a couple of examples:\n","\n","- The term \"giovanna\" has less weight in sentence 0 than in sentence 1, which was expected because sentence 0 has more words, resulting in a lower TF.\n","\n","- The term \"hop\" has more weight in sentence 0 compared to 'giovanna', which was expected because 'giovanna' is also present in sentence 1, resulting in a lower IDF.\n","\n","We can further enhance the TF-IDF approach by combining it with lemmatization or other preprocessing techniques already discussed, to represent words in a more concise and meaningful manner."],"metadata":{"id":"FwF27wFpsBlJ"}},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"id":"r6KJsVyKx_hp"}},{"cell_type":"markdown","source":["In this notebook, we have explored fundamental text representation techniques, starting with the simple yet effective Bag of Words (BoW) model and progressing to the more sophisticated TF-IDF approach. These methods have provided us with foundational tools for transforming text into numerical formats, which are essential for building initial models in Natural Language Processing (NLP).\n","\n","Looking ahead, we plan to explore more complex and powerful representation techniques, including word embeddings, which offer richer and more nuanced interpretations of text data. Now that we have established methods for representing text numerically, we are well-prepared to begin exploring simple NLP models, paving the way for more advanced analytical applications."],"metadata":{"id":"14Vw_cra0aUA"}}]}