{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Text Classification: Tree-based models"
      ],
      "metadata": {
        "id": "dup63p_hCz2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last time, we utilized a common machine learning algorithm, Logistic Regression, to perform text classification on a COVID-19 related dataset of tweets. Now, the goal is to present another widely used class of models for text classification: tree-based models. We aim to apply these models to the same data from the previous notebook."
      ],
      "metadata": {
        "id": "LtRn9mpMA03b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly import the preprocessed data again, so we can start experimenting with the new models!"
      ],
      "metadata": {
        "id": "jO7zlfTWCSpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_pickle('df_train.pkl')\n",
        "df_test = pd.read_pickle('df_test.pkl')"
      ],
      "metadata": {
        "id": "wsMc-fwdCSOs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we represent the text as vectors using TF-IDF and encode the labels as numbers."
      ],
      "metadata": {
        "id": "XOUx8TYuFEJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tQloHvk2Awdz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(df_train['ProcessedTweet'])\n",
        "y_train = label_encoder.fit_transform(df_train['Sentiment'])\n",
        "X_test = vectorizer.transform(df_test['ProcessedTweet'])\n",
        "y_test = label_encoder.transform(df_test['Sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "q8mZSis6Hwkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model we encounter is the Decision Tree. What is it? A Decision tree is one of the most interpretable machine learning algorithms. One can visualize a decision tree as a series of conditions (called nodes) that, starting from the data, lead to certain label (leaves). To easily explain the key concept behind the decision tree, consider a scenario in which the task is to predict if a person is an adult or a child and we have the height of that person. We could assert that, if a person's height is over 150 cm, that person could be predicted as an adult, and a child otherwise. In this case, the initial data is the height of the subject, the node (condition) is $\\mbox{height}>150 \\text{cm}$ and the labels are \"Adult\" and \"Child\". These labels also represent the leaves of the tree. During training, the algorithm finds the optimal conditions that best split the data to accurately classify each instance."
      ],
      "metadata": {
        "id": "fMNE1-UZHzXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the Decision Tree algorithm to our data!"
      ],
      "metadata": {
        "id": "6EglRRpKKU7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Test Classification Report: \\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHeJ8IwNFdWM",
        "outputId": "0fcc41e5-77ae-478e-e3f9-4ae6afa711d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.60      0.62      1633\n",
            "     Neutral       0.48      0.57      0.52       619\n",
            "    Positive       0.65      0.65      0.65      1546\n",
            "\n",
            "    accuracy                           0.61      3798\n",
            "   macro avg       0.59      0.61      0.60      3798\n",
            "weighted avg       0.62      0.61      0.62      3798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are quite disappointing.\n",
        "\n",
        "**Bonus**: can we improve the performance? This is an optimal scenario to introduce the concept of cross-validation!\n",
        "\n",
        "Until now, we have applied models with default hyperparameters (i.e., we don't specify anything when calling a model). Actually, we can try to modify these hyperparameters to see if some starting combinations work better than others! For the Decision Tree, we can tune these hyperparameters:\n",
        "\n",
        "- max_depth: This parameter sets the maximum depth of the tree. Limiting the depth of the tree helps to prevent overfitting. If set to None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "- min_samples_split: This parameter specifies the minimum number of samples required to split an internal node. A higher value can prevent the model from learning overly specific patterns in the training data, thus helping to reduce overfitting.\n",
        "\n",
        "- min_samples_leaf: This parameter sets the minimum number of samples required to be at a leaf node. A higher value can smooth the model, making it more resistant to noisy data.\n",
        "\n",
        "- max_features: This parameter specifies the maximum number of features to consider when looking for the best split. Limiting the number of features can introduce randomness into the model, helping to prevent overfitting.\n",
        "\n",
        "We can use the `GridSearchCV` object to define some values for the hyperparameters and check which combinations fit better. It performs an exhaustive search over specified hyperparameter values for a model. It uses cross-validation, which means splitting the original training data into multiple training and validation sets to evaluate the model's performance, to find the best hyperparameter combination before evaluating the performance on the test set."
      ],
      "metadata": {
        "id": "lHn0UYxRLlRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract the best combinations of hyperparameters with the method `.best_params_` and the best model with the method `.best_estimator_` and then evaluate the model on the test set."
      ],
      "metadata": {
        "id": "x4AR3AolUOl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell may take some time to be executed...\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 50],\n",
        "    'min_samples_split': [2, 10],\n",
        "    'min_samples_leaf': [1, 3, 7],\n",
        "    'max_features': [None, 'sqrt']\n",
        "}\n",
        "\n",
        "# Crea il modello Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Esegui la ricerca degli iperparametri\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Migliori parametri trovati\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Predizione sui dati di test con il miglior modello trovato\n",
        "best_dt = grid_search.best_estimator_\n",
        "y_pred = best_dt.predict(X_test)\n",
        "\n",
        "# Genera il classification report\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKtVxLjvO8sk",
        "outputId": "b04872fb-d0be-4a0c-8108-e08277db4899"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "Best parameters found:  {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.63      0.58      0.61      1633\n",
            "     Neutral       0.49      0.57      0.53       619\n",
            "    Positive       0.63      0.65      0.64      1546\n",
            "\n",
            "    accuracy                           0.61      3798\n",
            "   macro avg       0.58      0.60      0.59      3798\n",
            "weighted avg       0.61      0.61      0.61      3798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that, in this case, the best combination of hyperparameters among all the specified combinations is the default one. So, this model seems to have already reached its maximum performance, and we should consider other models for this task.\n"
      ],
      "metadata": {
        "id": "D5B0Cr6DPUaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One single tree seems to be inadequate for the problem. Why don't we use multiple trees? This is the underlying idea of three so-called ensemble algorithms: Bagging, Random Forest, Boosting."
      ],
      "metadata": {
        "id": "qREDep8UQgBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging"
      ],
      "metadata": {
        "id": "9Sifjm_oSaMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derived from Bootstrap Aggregating, the Bagging algorithm proposes to train multiple decision trees, each over a subset of the data (extracted with replacement). The final classification is then obtained as the majority class proposed by the group. We define a `BaggingClassifier` composed by 50 Decision Trees as an example."
      ],
      "metadata": {
        "id": "JQwa9qoVSbu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "base_classifier = DecisionTreeClassifier()\n",
        "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=50, random_state=1998)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz7PE_AcS-5h",
        "outputId": "73166813-44fd-4b28-c5bd-002e5ddeaa6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.70      0.67      0.69      1633\n",
            "     Neutral       0.58      0.67      0.62       619\n",
            "    Positive       0.72      0.70      0.71      1546\n",
            "\n",
            "    accuracy                           0.69      3798\n",
            "   macro avg       0.67      0.68      0.67      3798\n",
            "weighted avg       0.69      0.69      0.69      3798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that the performance is improved: United we stand, divided we fall. :-)"
      ],
      "metadata": {
        "id": "TsynNiPtVIUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "a7uY8UrXNsjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest creates a 'forest' of trees by randomly sampling both data points and features. Unlike Bagging, it also randomly selects a subset of features at each split. Each tree is trained on a different subset of the data, and the final prediction is made by majority vote from all the trees. The reason why Random Forest is preferred to Bagging is that it further reduces overfitting by introducing random feature selection at each split, leading to less correlated trees and typically improved model performance. We define a `RandomForestClassifier` composed by only 10 trees as an example."
      ],
      "metadata": {
        "id": "6btdyXaxREQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=1998)\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "2ryE15KCPUL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7acf8667-a3f3-4401-c340-7d3099d97efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:                precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.69      0.68      1633\n",
            "     Neutral       0.55      0.50      0.53       619\n",
            "    Positive       0.67      0.67      0.67      1546\n",
            "\n",
            "    accuracy                           0.65      3798\n",
            "   macro avg       0.63      0.62      0.63      3798\n",
            "weighted avg       0.65      0.65      0.65      3798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the performance seems worse compared to Bagging."
      ],
      "metadata": {
        "id": "7N8_FwouWrZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting"
      ],
      "metadata": {
        "id": "eoVyj5tLXxiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boosting algorithm sequentially trains a number of Decision Trees, where each tree attempts to correct the errors made by the previous ones. In the end, all the Decision Trees contribute to the final prediction, but the most recent ones typically have more influence. We define a `BoostingClassifier` composed by 50 trees as an example."
      ],
      "metadata": {
        "id": "cThQkdUqXxfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=1998)\n",
        "\n",
        "gradient_boosting_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gradient_boosting_classifier.predict(X_test)\n",
        "\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoIYmE1GWIrZ",
        "outputId": "bbd13f44-88b7-4ebb-f2c1-2f658514fb02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.53      0.62      1633\n",
            "     Neutral       0.62      0.22      0.32       619\n",
            "    Positive       0.53      0.84      0.65      1546\n",
            "\n",
            "    accuracy                           0.60      3798\n",
            "   macro avg       0.64      0.53      0.53      3798\n",
            "weighted avg       0.64      0.60      0.59      3798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the performance seems worse compared to Bagging."
      ],
      "metadata": {
        "id": "SDl9GZQgSh_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "YW_q-aqwMtI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have explored tree-based models as an additional resource beyond logistic regression for text classification. Starting with the Decision Tree model, we observed its simplicity and interpretability but also its limitations. To address these, we examined Bagging, which improves performance by aggregating multiple decision trees trained on different subsets of the data. Next, we looked into Random Forest, an enhancement of Bagging that further reduces overfitting by introducing random feature selection at each split, resulting in less correlated trees and improved model performance. Finally, we have explored boosting, which sequentially trains decision trees, each one focusing on correcting the errors of its predecessor. This method shows promise in achieving high predictive accuracy by iteratively refining the model. In future sessions, we will explore more advanced models such as embeddings and neural networks, which promise even greater capabilities for text classification tasks. These advanced techniques will help us further push the boundaries of what we can achieve with \"classical\" machine learning in text analytics."
      ],
      "metadata": {
        "id": "lqK0I1oTZgZZ"
      }
    }
  ]
}